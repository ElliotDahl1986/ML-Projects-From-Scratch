{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Elliot Dahl, November 2017\n",
    "\n",
    "This is a side project for me to learn more about Reinforcement learning.  \n",
    "\n",
    "I use Q-learning to teach an agent to pick up litter and avoid obstacles as an agent walks from one side of a grid to the other. Q-learning is model free learning technique that can be applied to find the best policy for a Markov decision process. More explicitly, it can be used to find the best action for each state, based on the state transition matrix. Originally this matrix is set to zero but through exploration of the state-action space and the possible rewards the agent learns it way around and can eventually decide an optimal route based on the given reward matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import random"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Agent class\n",
    "----------\n",
    "\n",
    "Here I define an agent class. The agent explores the state space using Gibbs policy improvement."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class state_action:\n",
    "    #Similar to a node\n",
    "    state = None\n",
    "    action = None\n",
    "    next_state = None\n",
    "\n",
    "#--------- Agent Class ---------------\n",
    "class agent:  \n",
    "\n",
    "    First = None #first state  \n",
    "    Current_state = None\n",
    "    \n",
    "    Q_mat_litter = None   \n",
    "    Q_mat_obstacles = None\n",
    "    Q_mat_across = None\n",
    "    \n",
    "    Act = np.array([0,1,2,3]) #action: 0=up, 1=right, 2=down, 3=left   \n",
    "    \n",
    "    def initiate_Q_all(self,number_states,number_actions):\n",
    "        self.Q_mat_litter = np.zeros((number_states,number_actions))\n",
    "        self.Q_mat_obstacles = np.zeros((number_states,number_actions))\n",
    "        self.Q_mat_across = np.zeros((number_states,number_actions))\n",
    "    \n",
    "    def zero_First_Current(self):\n",
    "        self.First = None\n",
    "        self.Current_state = None\n",
    "    \n",
    "    def choose_action(self,state,T,weight=np.array([1/3,1/3,1/3])):\n",
    "        Q_mat = (weight[0]*self.Q_mat_litter[state,:]+weight[1]*self.Q_mat_obstacles[state,:]\n",
    "                 +weight[2]*self.Q_mat_across[state,:])     \n",
    "        prob_a = explore_Gibb(Q_mat,T)\n",
    "        index_act = np.random.choice(self.Act,p=prob_a)\n",
    "        return index_act\n",
    "                \n",
    "    def update_Q_litter(self,state,action,next_state,Rew_mat,alpha=0.1,gamma=0.8):\n",
    "        r_state = Rew_mat[state]\n",
    "        q_state = self.Q_mat_litter[state,action]\n",
    "        q_new = q_state + alpha*(r_state+gamma*max(self.Q_mat_litter[next_state,:])-q_state)\n",
    "        self.Q_mat_litter[state,action] = q_new\n",
    "        \n",
    "    def update_Q_obstacles(self,state,action,next_state,Rew_mat,alpha=0.1,gamma=0.8):\n",
    "        r_state = Rew_mat[state]\n",
    "        q_state = self.Q_mat_obstacles[state,action]\n",
    "        q_new = q_state + alpha*(r_state+gamma*max(self.Q_mat_obstacles[next_state,:])-q_state)\n",
    "        self.Q_mat_obstacles[state,action] = q_new\n",
    "        \n",
    "    def update_Q_across(self,state,action,next_state,Rew_mat,alpha=0.1,gamma=0.8):\n",
    "        r_state = Rew_mat[state]\n",
    "        q_state = self.Q_mat_across[state,action]\n",
    "        q_new = q_state + alpha*(r_state+gamma*max(self.Q_mat_across[next_state,:])-q_state)\n",
    "        self.Q_mat_across[state,action] = q_new\n",
    "        \n",
    "    def save_state_action(self,state,action):\n",
    "        if self.First == None:\n",
    "            ny = state_action()\n",
    "            ny.state = state\n",
    "            ny.action = action\n",
    "            self.First = ny\n",
    "            self.Current_state = self.First                \n",
    "        else:\n",
    "            ny = state_action()\n",
    "            ny.state = state\n",
    "            ny.action = action\n",
    "            self.Current_state.next_state = ny\n",
    "            self.Current_state = ny\n",
    "                    \n",
    "#--------- Rules for updating the states ---------------               \n",
    "def next_state(state,action,row,col):\n",
    "    #action: 0=up\n",
    "    #state starts at 1\n",
    "    if action == 0:\n",
    "        if state < col: #can't go up from upper boundary\n",
    "            new_state = state\n",
    "        else:\n",
    "            new_state = state - col\n",
    "    #action: 1=right\n",
    "    if action == 1:\n",
    "        if (state+1)%col == 0:\n",
    "            new_state = state #can't take a right on the right boundary\n",
    "        else:\n",
    "            new_state = state+1\n",
    "    #action: 2=down\n",
    "    if action == 2:\n",
    "        if state + col >= row*col: #can't go down from lower boundary\n",
    "            new_state = state\n",
    "        else: \n",
    "            new_state = state + col\n",
    "    if action == 3:\n",
    "        if (state+1)%col == 1:\n",
    "            new_state = state\n",
    "        else:\n",
    "            new_state = state-1\n",
    "    return new_state\n",
    "\n",
    "def col_rows(state,row,col):\n",
    "    x_cord = state%col\n",
    "    y_cord = state//col\n",
    "    return x_cord,y_cord\n",
    "\n",
    "#--------- Enables exploration ---------------\n",
    "def explore_Gibb(Q,T):\n",
    "    pi_a = np.exp(Q/T)/np.sum(np.exp(Q/T))\n",
    "    return pi_a"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Environment\n",
    "-----------\n",
    "Here I set up the environment that I want the agent to learn from"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#--------- SET THE MODEL Parameters---------------\n",
    "size_col = 4\n",
    "size_rows = 10\n",
    "number_states = size_col*size_rows\n",
    "\n",
    "#Eaqual weight set to each module\n",
    "module_weight = np.array([1/3,1,1/3])\n",
    "\n",
    "N_episodes = 1e3\n",
    "N_litter_walk = 400\n",
    "\n",
    "#Assume actions are left,up,right,down- in this order in the state action table\n",
    "number_actions = 4\n",
    "\n",
    "#-------REWARD MATRICES----------\n",
    "#Go from top to bottom\n",
    "R_litter = np.zeros((size_rows,size_col))\n",
    "R_litter[1,2]=10\n",
    "R_litter[2,0]=10\n",
    "R_litter[3,3]=10\n",
    "R_litter[4,2]=10\n",
    "R_litter[5,0]=10\n",
    "R_litter[6,2]=10\n",
    "R_litter[7,2]=10\n",
    "R_litter[8,1]=10\n",
    "\n",
    "R_obstacles = np.zeros((size_rows,size_col))\n",
    "R_obstacles[1,1]=-10\n",
    "R_obstacles[2,1]=-10\n",
    "R_obstacles[3,2]=-10\n",
    "R_obstacles[4,0]=-10\n",
    "R_obstacles[5,3]=-10\n",
    "R_obstacles[6,1]=-10\n",
    "R_obstacles[7,3]=-10\n",
    "R_obstacles[8,0]=-10\n",
    "R_obstacles[8,2]=-10\n",
    "\n",
    "R_across = np.zeros((size_rows,size_col))\n",
    "R_across[-1,:] = 10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Main\n",
    "----\n",
    "Creating an agent and learning the environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#--------- Main ---------------    \n",
    "#Create the litter agent\n",
    "agent_comb = agent()\n",
    "agent_comb.initiate_Q_all(number_states,number_actions)\n",
    "\n",
    "R_obstacles_copy = np.reshape(R_obstacles.copy(),number_states) \n",
    "R_across_copy = np.reshape(R_across.copy(),number_states) \n",
    "\n",
    "for i in range(int(N_episodes)):\n",
    "    agent_comb.zero_First_Current() #Have to set these to zero each episode\n",
    "    R_litter_copy = np.reshape(R_litter.copy(),number_states) #Since the agent picks up the trash\n",
    "    \n",
    "    initial_state = random.choice(np.arange(0,size_col)) #Random start on first row\n",
    "    action = agent_comb.choose_action(initial_state,N_episodes/(i+1),module_weight)\n",
    "    \n",
    "    agent_comb.save_state_action(initial_state,action)       \n",
    "    new_state = next_state(initial_state,action,size_rows,size_col)\n",
    "    \n",
    "    #Update the three Q matrices    \n",
    "    agent_comb.update_Q_litter(initial_state,action,new_state,R_litter_copy)\n",
    "    agent_comb.update_Q_obstacles(initial_state,action,new_state,R_obstacles_copy)\n",
    "    agent_comb.update_Q_across(initial_state,action,new_state,R_across_copy)\n",
    "  \n",
    "    if R_litter_copy[initial_state] != 0: #Pick up trash\n",
    "        R_litter_copy[initial_state] = 0\n",
    "    \n",
    "    for k in range(N_litter_walk):\n",
    "        \n",
    "        action = agent_comb.choose_action(new_state,N_episodes/(i+1),module_weight)\n",
    "        agent_comb.save_state_action(new_state,action)\n",
    "        \n",
    "        new_state = next_state(agent_comb.Current_state.state,action,size_rows,size_col)\n",
    "        \n",
    "        #Update the three Q matrices    \n",
    "        agent_comb.update_Q_litter(agent_comb.Current_state.state,action,new_state,R_litter_copy)\n",
    "        agent_comb.update_Q_obstacles(agent_comb.Current_state.state,action,new_state,R_obstacles_copy)\n",
    "        agent_comb.update_Q_across(agent_comb.Current_state.state,action,new_state,R_across_copy)        \n",
    "        \n",
    "        if R_litter_copy[agent_comb.Current_state.state] != 0:\n",
    "            R_litter_copy[agent_comb.Current_state.state] = 0\n",
    "        if agent_comb.Current_state.state>number_states-size_col-1:\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAARQAAAKBCAYAAACWM1QUAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nBAVoIyhAG0EB2ggK0EZQgDb/H1wdIUbOR2D+AAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x10fb222e8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#--------- Plot the Results ---------------\n",
    "elem = agent_comb.First\n",
    "counter = 1\n",
    "while True:\n",
    "    elem = elem.next_state\n",
    "    counter +=1\n",
    "    if elem.next_state is None:\n",
    "        break\n",
    "\n",
    "x_agent = np.zeros(counter)\n",
    "y_agent = np.zeros(counter)\n",
    "actions_agent = np.zeros(counter)\n",
    "\n",
    "elem = agent_comb.First\n",
    "k = 0\n",
    "while True:\n",
    "    i,j = col_rows(elem.state,size_rows,size_col)\n",
    "    x_agent[k]=i\n",
    "    y_agent[k]=j\n",
    "    actions_agent[k]=elem.action\n",
    "    k += 1\n",
    "    if elem.next_state is None:\n",
    "        break\n",
    "    elem = elem.next_state  \n",
    "\n",
    "R_full = R_litter+R_obstacles+R_across\n",
    "\n",
    "fig=plt.figure(figsize=(12, 10), dpi= 80, edgecolor='k')\n",
    "plt.imshow(R_full,interpolation='nearest')        \n",
    "plt.plot(x_agent,y_agent)\n",
    "plt.scatter(x_agent,y_agent, marker='+', s=150, linewidths=4, c=np.arange(0,counter), cmap=plt.cm.coolwarm);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The agent starts from the top, blue cross, while it ends at the bottom, red cross. The yellow corresponds to a reward of 10. Blue corresponds to a punishment of -10. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
